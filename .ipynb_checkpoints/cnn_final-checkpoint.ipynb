{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create labels\n",
    "Input: Folder Path   \n",
    "Output: Tuple (Label, Indices of the labels, one-hot encoded labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./eng_data/\"\n",
    "\n",
    "def get_labels(path=DATA_PATH):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting wave to mfcc\n",
    "Input: path of file, maximum pad length(default=11)   \n",
    "Output: list of mfcc vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2mfcc(file_path, max_len=11):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = wave[::3]\n",
    "    try:\n",
    "        mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "    except Exception as e:\n",
    "        print(file_path)\n",
    "        print(e)\n",
    "        \n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving mfcc vectors to .npy files\n",
    "Input: path to write files   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_array(path=DATA_PATH, max_len=11):\n",
    "    labels, _, _ = get_labels(path)\n",
    "\n",
    "    for label in labels:\n",
    "        mfcc_vectors = []\n",
    "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "        \n",
    "        for wavfile in tqdm(wavfiles, \"Saving vectors of label - '{}'\".format(label)):\n",
    "            mfcc = wav2mfcc(wavfile, max_len=max_len)\n",
    "            mfcc_vectors.append(mfcc)\n",
    "            \n",
    "        np.save(label + '.npy', mfcc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing data into training set and testing set\n",
    "Input: split ratio (default=0.8)    \n",
    "Output: 2 arrays of training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(split_ratio=0.8):\n",
    "    # Get available labels\n",
    "    labels, indices, _ = get_labels(DATA_PATH)\n",
    "\n",
    "    # Getting first arrays\n",
    "    X = np.load(labels[0] + '.npy')\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load(label + '.npy')\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value= (i+1)))\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    return train_test_split(X, y, test_size= (1 - split_ratio), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization by calling the necessay functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Second dimension of the feature is dim2\n",
    "feature_dim_2 = 11\n",
    "\n",
    "# Save data to array file first (Comment next line when npy files are created to save time)\n",
    "# save_data_to_array(max_len=feature_dim_2)\n",
    "\n",
    "# Loading train set and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "# Feature dimension\n",
    "feature_dim_1 = 20\n",
    "channel = 1\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts a sample\n",
    "def predict(filepath, model):\n",
    "    sample = wav2mfcc(filepath)\n",
    "    sample_reshaped = sample.reshape(1, feature_dim_1, feature_dim_2, channel)\n",
    "    return get_labels()[0][\n",
    "            np.argmax(model.predict(sample_reshaped))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13256 samples, validate on 3314 samples\n",
      "Epoch 1/20\n",
      "13256/13256 [==============================] - 16s 1ms/step - loss: 1.8631 - acc: 0.3349 - val_loss: 1.2017 - val_acc: 0.5851\n",
      "Epoch 2/20\n",
      "13256/13256 [==============================] - 14s 1ms/step - loss: 1.0380 - acc: 0.6321 - val_loss: 0.6030 - val_acc: 0.8014\n",
      "Epoch 3/20\n",
      "13256/13256 [==============================] - 14s 1ms/step - loss: 0.6962 - acc: 0.7657 - val_loss: 0.4850 - val_acc: 0.8401\n",
      "Epoch 4/20\n",
      "13256/13256 [==============================] - 14s 1ms/step - loss: 0.5202 - acc: 0.8245 - val_loss: 0.3293 - val_acc: 0.8887\n",
      "Epoch 5/20\n",
      "13256/13256 [==============================] - 15s 1ms/step - loss: 0.4095 - acc: 0.8662 - val_loss: 0.3212 - val_acc: 0.8956\n",
      "Epoch 6/20\n",
      "13256/13256 [==============================] - 14s 1ms/step - loss: 0.3369 - acc: 0.8910 - val_loss: 0.3330 - val_acc: 0.8890\n",
      "Epoch 7/20\n",
      "13256/13256 [==============================] - 14s 1ms/step - loss: 0.2900 - acc: 0.9030 - val_loss: 0.2692 - val_acc: 0.9155\n",
      "Epoch 8/20\n",
      "13256/13256 [==============================] - 14s 1ms/step - loss: 0.2480 - acc: 0.9172 - val_loss: 0.2508 - val_acc: 0.9206\n",
      "Epoch 9/20\n",
      "13256/13256 [==============================] - 14s 1ms/step - loss: 0.2143 - acc: 0.9279 - val_loss: 0.3194 - val_acc: 0.8977\n",
      "Epoch 10/20\n",
      "13256/13256 [==============================] - 12s 905us/step - loss: 0.1953 - acc: 0.9353 - val_loss: 0.3155 - val_acc: 0.9119\n",
      "Epoch 11/20\n",
      "13256/13256 [==============================] - 12s 873us/step - loss: 0.1775 - acc: 0.9396 - val_loss: 0.3216 - val_acc: 0.9022\n",
      "Epoch 12/20\n",
      "13256/13256 [==============================] - 12s 870us/step - loss: 0.1573 - acc: 0.9498 - val_loss: 0.2831 - val_acc: 0.9179\n",
      "Epoch 13/20\n",
      "13256/13256 [==============================] - 11s 828us/step - loss: 0.1478 - acc: 0.9522 - val_loss: 0.2595 - val_acc: 0.9194\n",
      "Epoch 14/20\n",
      "13256/13256 [==============================] - 11s 833us/step - loss: 0.1269 - acc: 0.9586 - val_loss: 0.3155 - val_acc: 0.9125\n",
      "Epoch 15/20\n",
      "13256/13256 [==============================] - 13s 1ms/step - loss: 0.1098 - acc: 0.9648 - val_loss: 0.3003 - val_acc: 0.9143\n",
      "Epoch 16/20\n",
      "13256/13256 [==============================] - 13s 951us/step - loss: 0.1126 - acc: 0.9648 - val_loss: 0.3203 - val_acc: 0.9119\n",
      "Epoch 17/20\n",
      "13256/13256 [==============================] - 14s 1ms/step - loss: 0.0934 - acc: 0.9699 - val_loss: 0.2975 - val_acc: 0.9276\n",
      "Epoch 18/20\n",
      "13256/13256 [==============================] - 11s 836us/step - loss: 0.0979 - acc: 0.9688 - val_loss: 0.2990 - val_acc: 0.9249\n",
      "Epoch 19/20\n",
      "13256/13256 [==============================] - 11s 835us/step - loss: 0.0849 - acc: 0.9716 - val_loss: 0.3411 - val_acc: 0.9200\n",
      "Epoch 20/20\n",
      "13256/13256 [==============================] - 11s 832us/step - loss: 0.0847 - acc: 0.9732 - val_loss: 0.3200 - val_acc: 0.9212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc21b576588>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.fit(X_train, y_train_hot, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_test, y_test_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 19, 10, 32)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 9, 48)         6192      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 17, 8, 120)        23160     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 4, 120)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 4, 120)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3840)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               491648    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 529,871\n",
      "Trainable params: 529,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_built': True,\n",
      " '_collected_trainable_weights': [<tf.Variable 'conv2d_1/kernel:0' shape=(2, 2, 1, 32) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'conv2d_1/bias:0' shape=(32,) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'conv2d_2/kernel:0' shape=(2, 2, 32, 48) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'conv2d_2/bias:0' shape=(48,) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'conv2d_3/kernel:0' shape=(2, 2, 48, 120) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'conv2d_3/bias:0' shape=(120,) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'dense_1/kernel:0' shape=(3840, 128) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'dense_2/kernel:0' shape=(128, 64) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'dense_3/kernel:0' shape=(64, 7) dtype=float32_ref>,\n",
      "                                  <tf.Variable 'dense_3/bias:0' shape=(7,) dtype=float32_ref>],\n",
      " '_compute_previous_mask': True,\n",
      " '_expects_training_arg': False,\n",
      " '_feed_input_names': ['conv2d_1_input'],\n",
      " '_feed_input_shapes': [(None, 20, 11, 1)],\n",
      " '_feed_inputs': [<tf.Tensor 'conv2d_1_input:0' shape=(?, 20, 11, 1) dtype=float32>],\n",
      " '_feed_loss_fns': [<function categorical_crossentropy at 0x7fc21b6c1378>],\n",
      " '_feed_output_names': ['dense_3'],\n",
      " '_feed_output_shapes': [(None, 7)],\n",
      " '_feed_outputs': [<tf.Tensor 'dense_3/Softmax:0' shape=(?, 7) dtype=float32>],\n",
      " '_feed_sample_weight_modes': [None],\n",
      " '_feed_sample_weights': [<tf.Tensor 'dense_3_sample_weights:0' shape=(?,) dtype=float32>],\n",
      " '_feed_targets': [<tf.Tensor 'dense_3_target:0' shape=(?, ?) dtype=float32>],\n",
      " '_function_kwargs': {},\n",
      " '_inbound_nodes': [<keras.engine.base_layer.Node object at 0x7fc26807e9b0>],\n",
      " '_initial_weights': None,\n",
      " '_input_coordinates': [(<keras.engine.input_layer.InputLayer object at 0x7fc21b576fd0>,\n",
      "                         0,\n",
      "                         0)],\n",
      " '_input_layers': [<keras.engine.input_layer.InputLayer object at 0x7fc21b576fd0>],\n",
      " '_is_compiled': True,\n",
      " '_is_graph_network': True,\n",
      " '_layers': [<keras.engine.input_layer.InputLayer object at 0x7fc21b576fd0>,\n",
      "             <keras.layers.convolutional.Conv2D object at 0x7fc21b576e80>,\n",
      "             <keras.layers.convolutional.Conv2D object at 0x7fc26807e828>,\n",
      "             <keras.layers.convolutional.Conv2D object at 0x7fc21b53fac8>,\n",
      "             <keras.layers.pooling.MaxPooling2D object at 0x7fc21b4e3358>,\n",
      "             <keras.layers.core.Dropout object at 0x7fc21b4e37b8>,\n",
      "             <keras.layers.core.Flatten object at 0x7fc21b4e3780>,\n",
      "             <keras.layers.core.Dense object at 0x7fc21b501208>,\n",
      "             <keras.layers.core.Dropout object at 0x7fc21b4c3fd0>,\n",
      "             <keras.layers.core.Dense object at 0x7fc21b4732b0>,\n",
      "             <keras.layers.core.Dropout object at 0x7fc21b473dd8>,\n",
      "             <keras.layers.core.Dense object at 0x7fc21b473ef0>],\n",
      " '_layers_by_depth': {0: [<keras.layers.core.Dense object at 0x7fc21b473ef0>],\n",
      "                      1: [<keras.layers.core.Dropout object at 0x7fc21b473dd8>],\n",
      "                      2: [<keras.layers.core.Dense object at 0x7fc21b4732b0>],\n",
      "                      3: [<keras.layers.core.Dropout object at 0x7fc21b4c3fd0>],\n",
      "                      4: [<keras.layers.core.Dense object at 0x7fc21b501208>],\n",
      "                      5: [<keras.layers.core.Flatten object at 0x7fc21b4e3780>],\n",
      "                      6: [<keras.layers.core.Dropout object at 0x7fc21b4e37b8>],\n",
      "                      7: [<keras.layers.pooling.MaxPooling2D object at 0x7fc21b4e3358>],\n",
      "                      8: [<keras.layers.convolutional.Conv2D object at 0x7fc21b53fac8>],\n",
      "                      9: [<keras.layers.convolutional.Conv2D object at 0x7fc26807e828>],\n",
      "                      10: [<keras.layers.convolutional.Conv2D object at 0x7fc21b576e80>],\n",
      "                      11: [<keras.engine.input_layer.InputLayer object at 0x7fc21b576fd0>]},\n",
      " '_losses': [],\n",
      " '_network_nodes': {'conv2d_1_ib-0',\n",
      "                    'conv2d_1_input_ib-0',\n",
      "                    'conv2d_2_ib-0',\n",
      "                    'conv2d_3_ib-0',\n",
      "                    'dense_1_ib-0',\n",
      "                    'dense_2_ib-0',\n",
      "                    'dense_3_ib-0',\n",
      "                    'dropout_1_ib-0',\n",
      "                    'dropout_2_ib-0',\n",
      "                    'dropout_3_ib-0',\n",
      "                    'flatten_1_ib-0',\n",
      "                    'max_pooling2d_1_ib-0'},\n",
      " '_nodes_by_depth': {0: [<keras.engine.base_layer.Node object at 0x7fc21b1ffba8>],\n",
      "                     1: [<keras.engine.base_layer.Node object at 0x7fc21b435cf8>],\n",
      "                     2: [<keras.engine.base_layer.Node object at 0x7fc21b435e80>],\n",
      "                     3: [<keras.engine.base_layer.Node object at 0x7fc21b473c18>],\n",
      "                     4: [<keras.engine.base_layer.Node object at 0x7fc21b473f98>],\n",
      "                     5: [<keras.engine.base_layer.Node object at 0x7fc21b4c3c50>],\n",
      "                     6: [<keras.engine.base_layer.Node object at 0x7fc26cb597b8>],\n",
      "                     7: [<keras.engine.base_layer.Node object at 0x7fc21b501e10>],\n",
      "                     8: [<keras.engine.base_layer.Node object at 0x7fc21b501fd0>],\n",
      "                     9: [<keras.engine.base_layer.Node object at 0x7fc21b53fe10>],\n",
      "                     10: [<keras.engine.base_layer.Node object at 0x7fc21b5867f0>],\n",
      "                     11: [<keras.engine.base_layer.Node object at 0x7fc26807e898>]},\n",
      " '_outbound_nodes': [],\n",
      " '_output_coordinates': [(<keras.layers.core.Dense object at 0x7fc21b473ef0>,\n",
      "                          0,\n",
      "                          0)],\n",
      " '_output_layers': [<keras.layers.core.Dense object at 0x7fc21b473ef0>],\n",
      " '_output_mask_cache': {'140471659162928_93875552373552': None},\n",
      " '_output_shape_cache': {},\n",
      " '_output_tensor_cache': {},\n",
      " '_per_input_losses': {},\n",
      " '_per_input_updates': {},\n",
      " '_updates': [],\n",
      " '_uses_inputs_arg': True,\n",
      " 'history': <keras.callbacks.History object at 0x7fc21b576588>,\n",
      " 'input_names': ['conv2d_1_input'],\n",
      " 'inputs': [<tf.Tensor 'conv2d_1_input:0' shape=(?, 20, 11, 1) dtype=float32>],\n",
      " 'loss': <function categorical_crossentropy at 0x7fc21b6c1378>,\n",
      " 'loss_functions': [<function categorical_crossentropy at 0x7fc21b6c1378>],\n",
      " 'loss_weights': None,\n",
      " 'metrics': ['accuracy'],\n",
      " 'metrics_names': ['loss', 'acc'],\n",
      " 'metrics_tensors': [<tf.Tensor 'metrics/acc/Mean:0' shape=() dtype=float32>],\n",
      " 'metrics_updates': [],\n",
      " 'name': 'sequential_1',\n",
      " 'optimizer': <keras.optimizers.Adadelta object at 0x7fc21b1db390>,\n",
      " 'output_names': ['dense_3'],\n",
      " 'outputs': [<tf.Tensor 'dense_3/Softmax:0' shape=(?, 7) dtype=float32>],\n",
      " 'predict_function': <keras.backend.tensorflow_backend.Function object at 0x7fc21810e978>,\n",
      " 'sample_weight_mode': None,\n",
      " 'sample_weight_modes': [None],\n",
      " 'sample_weights': [<tf.Tensor 'dense_3_sample_weights:0' shape=(?,) dtype=float32>],\n",
      " 'stateful_metric_functions': [],\n",
      " 'stateful_metric_names': [],\n",
      " 'stop_training': False,\n",
      " 'supports_masking': False,\n",
      " 'targets': [<tf.Tensor 'dense_3_target:0' shape=(?, ?) dtype=float32>],\n",
      " 'test_function': <keras.backend.tensorflow_backend.Function object at 0x7fc21b1db080>,\n",
      " 'total_loss': <tf.Tensor 'loss/mul:0' shape=() dtype=float32>,\n",
      " 'train_function': <keras.backend.tensorflow_backend.Function object at 0x7fc21b1ad5c0>,\n",
      " 'trainable': True,\n",
      " 'weighted_metrics': None}\n"
     ]
    }
   ],
   "source": [
    "# A nice method to check all the attributes of an object\n",
    "from pprint import pprint\n",
    "pprint(vars(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n"
     ]
    }
   ],
   "source": [
    "print(predict('./eng_data/left/1cc80e39_nohash_0.wav', model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(filepath, model):\n",
    "    sample = wav2mfcc(filepath)\n",
    "    sample_reshaped = sample.reshape(1, feature_dim_1, feature_dim_2, channel)\n",
    "    u = model.predict(sample_reshaped)\n",
    "#     print(u)\n",
    "    return u, get_labels()[0][\n",
    "            np.argmax(model.predict(sample_reshaped))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = get_labels()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down      8.978884e-09\n",
      "go      8.8394725e-10\n",
      "left      0.99993\n",
      "on      1.9132768e-08\n",
      "right      6.860813e-05\n",
      "up      4.0531023e-07\n",
      "yes      9.4027564e-07\n",
      "left\n"
     ]
    }
   ],
   "source": [
    "confs, word = confidence('./eng_data/left/1cc80e39_nohash_0.wav', model=model)\n",
    "for lab,conf in zip(labs,confs[0]):\n",
    "    print(lab , \"    \" ,conf)\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(new_sample, model):\n",
    "\n",
    "    new_sample = new_sample.reshape(-1)\n",
    "    # print(\"Ye: \" + str(new_sample.shape))\n",
    "    sample = array2mfcc(new_sample)\n",
    "    sample_reshaped = sample.reshape(1,20,11,1)\n",
    "    return model.predict(sample_reshaped)\n",
    "\n",
    "\n",
    "def get_conf(inp, model):\n",
    "    labs = get_labels()[0]\n",
    "    inp = np.array(inp)\n",
    "    confs = confidence(inp, model=model)\n",
    "#     print(\"zzz\")\n",
    "#     print(confs[0])\n",
    "#     print(labs)\n",
    "    for lab,conf in zip(labs,confs[0]):\n",
    "        print(lab , \"    \" ,conf)\n",
    "    # print(word)\n",
    "    \n",
    "def get_conf2(sample, model):\n",
    "    for a,b in zip(get_labels()[0], model.predict(sample)[0]):\n",
    "        print(a, \"     \", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.701204\n",
      "0.037764013\n",
      "0.004798433\n",
      "0.08345175\n",
      "0.00975744\n",
      "0.007902366\n",
      "0.15512204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.701204  , 0.03776401, 0.00479843, 0.08345175, 0.00975744,\n",
       "       0.00790237, 0.15512204], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in model.predict(sample_reshaped)[0]:\n",
    "#     print(i)\n",
    "# model.predict(sample_reshaped)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model in realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the recorded array to mfcc\n",
    "Instead of recording speech into a file and reading from it to create an array, this function would directly convert the recorded array into mfcc coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2mfcc(wave, max_len=11):\n",
    "    sr = 16000\n",
    "    wave = wave[::3]\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down', 'go', 'left', 'on', 'right', 'up', 'yes']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For reference: These are the words trained to the classifier \n",
    "get_labels()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording...\n",
    "Please speak after executing the next cell.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 1  # seconds\n",
    "fs = 16000\n",
    "new_sample = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "time.sleep(duration)\n",
    "sd.play(new_sample, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "down       0.09955799\n",
      "go       0.0428727\n",
      "left       0.15488596\n",
      "on       0.007816722\n",
      "right       0.012278808\n",
      "up       0.0067787645\n",
      "yes       0.675809\n"
     ]
    }
   ],
   "source": [
    "new_sample = new_sample.reshape(-1)\n",
    "sample = array2mfcc(new_sample)\n",
    "sample_reshaped = sample.reshape(1,20,11,1)\n",
    "\n",
    "print(get_labels()[0][np.argmax(model.predict(sample_reshaped))])\n",
    "get_conf2(sample_reshaped, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-118-ebe35fe59f44>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-118-ebe35fe59f44>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ..stop..\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "..stop.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model7.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model7.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model1.json', 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"model1.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
